{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -q openai azure-core azure-search-documents azure-storage-blob tika"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run Documentum/logging_utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import uuid\n",
        "import json\n",
        "from openai import AzureOpenAI\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.storage.blob import BlobServiceClient \n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from azure.search.documents.indexes.models import (\n",
        "    SearchIndex,\n",
        "    SearchField,\n",
        "    SearchFieldDataType,\n",
        "    SimpleField,\n",
        "    SearchableField,\n",
        "    SearchIndex,\n",
        "    SearchField,\n",
        "    VectorSearch,\n",
        "    VectorSearchProfile,\n",
        "    HnswAlgorithmConfiguration,\n",
        "    SemanticSearch,\n",
        "    SemanticConfiguration,\n",
        "    SemanticPrioritizedFields,\n",
        "    SemanticField\n",
        ")\n",
        "from azure.storage.blob import (\n",
        "\n",
        "    BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
        "\n",
        ")\n",
        "\n",
        "import textwrap\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from tika import parser  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "service_endpoint = \"\"\n",
        "index_name       = \"\"\n",
        "key              = \"\"\n",
        "credential       = AzureKeyCredential(key)\n",
        "vector_dims       = 1536\n",
        "algo_name         = \"hnsw-cosine\"\n",
        "profile_name      = \"openai-ada-profile\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the index fields\n",
        "client = SearchIndexClient(service_endpoint, credential)\n",
        "fields = [\n",
        "    SimpleField(name=\"id\",   type=SearchFieldDataType.String,key=True, sortable=True, filterable=True, facetable=True),\n",
        "    SimpleField(name=\"file_name\", type=SearchFieldDataType.String),\n",
        "    \n",
        "    SimpleField(name=\"page_number\",type=SearchFieldDataType.Int32,sortable=True,filterable=True,facetable=False),\n",
        "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"content_vector\",type=SearchFieldDataType.Collection(SearchFieldDataType.Single),searchable=True,vector_search_dimensions=1536,vector_search_profile_name=\"my-vector-config\"),\n",
        "    SimpleField(name=\"storage_url\",type=SearchFieldDataType.String,filterable=False,facetable=False,sortable=False)\n",
        "]\n",
        " \n",
        "vector_search = VectorSearch(\n",
        "    profiles=[VectorSearchProfile(name=\"my-vector-config\",\n",
        "                                algorithm_configuration_name=\"my-algorithms-config\")],\n",
        "    algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithms-config\")],\n",
        ")\n",
        "\n",
        "# Define semantic configuration\n",
        "semantic_config = SemanticConfiguration(\n",
        "    name=\"semantic-config\",\n",
        "    prioritized_fields=SemanticPrioritizedFields(\n",
        "        title_field=SemanticField(field_name=\"file_name\"),\n",
        "        content_fields=[SemanticField(field_name=\"content\")]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add semantic search to the index\n",
        "semantic_search = SemanticSearch(\n",
        "    default_configuration_name=\"semantic-config\",\n",
        "    configurations=[semantic_config]\n",
        ")\n",
        "\n",
        "# Update the index definition\n",
        "index = SearchIndex(\n",
        "    name=index_name,\n",
        "    fields=fields,\n",
        "    vector_search=vector_search,\n",
        "    semantic_search=semantic_search\n",
        ")\n",
        "\n",
        "client.create_or_update_index(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ─── 1. CONFIG ────────────────────────────────────────────────────────────────\n",
        "AZURE_OPENAI_ENDPOINT  = \"\"\n",
        "AZURE_OPENAI_KEY       = \"\"\n",
        "AZURE_OPENAI_API_VER   = \"2024-10-21\"\n",
        "EMBED_DEPLOYMENT       = \"\"\n",
        " \n",
        "SEARCH_ENDPOINT        = \"\"\n",
        "SEARCH_API_KEY         = \"\"\n",
        "SEARCH_INDEX_NAME      = \"jsonidx\"\n",
        " \n",
        "ADLS_CONNECTION_STRING = \"\"  # blob endpoint\n",
        "ADLS_CONTAINER_NAME    = \"raw\"\n",
        "ADLS_DIR_PREFIX        = \"doc/\"\n",
        "ADLS_STORAGE_ACCOUNT  = \"czvgngendif000000dsta001\"\n",
        "ADLS_STORAGE_KEY = \"\"\n",
        "\n",
        " \n",
        "TEXT_MIN_CHARS         = 5                 # too-short chunks are skipped\n",
        "\n",
        "CHUNK_SIZE             = 2_000               # ≈ 700–800 tokens\n",
        " \n",
        "# ─── 2. CLIENTS ───────────────────────────────────────────────────────────────\n",
        " \n",
        "openai_client = AzureOpenAI(\n",
        "\n",
        "    api_key        = AZURE_OPENAI_KEY,\n",
        "\n",
        "    azure_endpoint = AZURE_OPENAI_ENDPOINT,\n",
        "\n",
        "    api_version    = AZURE_OPENAI_API_VER,\n",
        "\n",
        ")\n",
        " \n",
        "search_client = SearchClient(\n",
        "\n",
        "    endpoint   = SEARCH_ENDPOINT,\n",
        "\n",
        "    index_name = SEARCH_INDEX_NAME,\n",
        "\n",
        "    credential = AzureKeyCredential(SEARCH_API_KEY)\n",
        "\n",
        ")\n",
        " \n",
        "blob_service = BlobServiceClient.from_connection_string(ADLS_CONNECTION_STRING)\n",
        "\n",
        "container     = blob_service.get_container_client(ADLS_CONTAINER_NAME)\n",
        " \n",
        "# ─── 3. HELPERS ───────────────────────────────────────────────────────────────\n",
        "             # downloads Apache Tika JAR at first call\n",
        " \n",
        "def create_embedding(text: str, model: str = EMBED_DEPLOYMENT) -> list[float]:\n",
        "\n",
        "    \"\"\"Return an embedding vector for `text`.\"\"\"\n",
        "\n",
        "    return openai_client.embeddings.create(\n",
        "\n",
        "        input=[text], model=model\n",
        "\n",
        "    ).data[0].embedding\n",
        " \n",
        "def doc_chunks_from_bytes(doc_bytes: bytes, *, size: int = CHUNK_SIZE):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Yield (chunk_no, text) tuples for the .doc binary.\n",
        "\n",
        "    Chunks are plain-text slices of ≈`size` characters.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    raw = parser.from_buffer(doc_bytes).get(\"content\", \"\") or \"\"\n",
        "\n",
        "    clean = \" \".join(raw.split())        # collapse whitespace/newlines\n",
        "\n",
        "    for i in range(0, len(clean), size):\n",
        "\n",
        "        chunk = clean[i : i + size].strip()\n",
        "\n",
        "        if chunk:\n",
        "\n",
        "            yield (i // size + 1), chunk\n",
        " \n",
        "def make_sas_url(blob_name: str) -> str:\n",
        "\n",
        "    sas = generate_blob_sas(\n",
        "\n",
        "        account_name = ADLS_STORAGE_ACCOUNT,\n",
        "\n",
        "        container_name = ADLS_CONTAINER_NAME,\n",
        "\n",
        "        blob_name = blob_name,\n",
        "\n",
        "        account_key = ADLS_STORAGE_KEY,\n",
        "\n",
        "        permission = BlobSasPermissions(read=True),\n",
        "\n",
        "        expiry = datetime.utcnow() + timedelta(days=7),\n",
        "\n",
        "    )\n",
        "\n",
        "    return (\n",
        "\n",
        "        f\"https://{ADLS_STORAGE_ACCOUNT}.blob.core.usgovcloudapi.net/\"\n",
        "\n",
        "        f\"{ADLS_CONTAINER_NAME}/{blob_name}?{sas}\"\n",
        "\n",
        "    )\n",
        " \n",
        "# ─── 4. MAIN ──────────────────────────────────────────────────────────────────\n",
        "\n",
        "def main():\n",
        "\n",
        "    uploaded = skipped = 0\n",
        "    total_doc_parsed = 0\n",
        "    total_doc = 0\n",
        "    page_parsed = 0\n",
        "    doc_size = 0\n",
        "    start = datetime.utcnow()\n",
        " \n",
        "    print(\"➜ Scanning ADLS container for .doc files …\")\n",
        "    for blob in container.list_blobs(name_starts_with=ADLS_DIR_PREFIX):\n",
        "        if not blob.name.lower().endswith(\".doc\"):      # legacy Office format\n",
        "            continue\n",
        "        print(f\"\\n➜ Processing {blob.name}\")\n",
        "        doc_bytes = container.download_blob(blob.name).readall()\n",
        "        total_doc += 1\n",
        "        doc_size += len(doc_bytes)\n",
        "        for chunk_no, text in doc_chunks_from_bytes(doc_bytes):\n",
        "            if len(text) < TEXT_MIN_CHARS:\n",
        "                skipped += 1\n",
        "                continue\n",
        "            record = {\n",
        "                \"id\"            : str(uuid.uuid4()),\n",
        "                \"file_name\"     : blob.name,\n",
        "                \"page_number\"   : chunk_no,             # “page” == chunk index\n",
        "                \"content\"       : text,\n",
        "                \"content_vector\": create_embedding(text),\n",
        "                \"storage_url\"   : make_sas_url(blob.name),\n",
        "            }\n",
        "            result = search_client.upload_documents([record])[0]\n",
        "            status = \"✓\" if result.succeeded else \"✗\"\n",
        "            if(status == \"✓\"):\n",
        "                page_parsed += 1\n",
        "            uploaded += int(result.succeeded)\n",
        "        if(page_parsed > 0):\n",
        "            total_doc_parsed +=1\n",
        "        print(f\"Total page parsed {page_parsed}\")\n",
        "        page_parsed = 0\n",
        " \n",
        "    print(f\"\\nDone. {uploaded} chunks indexed, {skipped} chunks skipped.\")\n",
        "    print(f\"Total DOCX processed: {total_doc_parsed}\\n Total DOCX in Raw: {total_doc}\" )\n",
        "    end = datetime.utcnow()\n",
        "    pid = str(uuid.uuid4())\n",
        "    stage = \"Bronze to Silver\"\n",
        "    status = \"Convert Doc to Ai\"\n",
        "    size = doc_size\n",
        "    log_activity(pid, stage, status,start,end, size)\n",
        "    \n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        " "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
