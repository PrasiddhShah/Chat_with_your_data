{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import adal, datetime, json, re, time\n",
        "from delta.tables import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run Functions/ReferenceFunctions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sql_inserts(df, table_name, stmt):\n",
        "    insert_statements = []\n",
        "    for idx, row in df.iterrows():\n",
        "        columns = ', '.join(f\"[{col}]\" for col in row.index)\n",
        "        values = []\n",
        "        for val in row.values:\n",
        "            # Handle booleans\n",
        "            print(val)\n",
        "            if isinstance(val, bool):\n",
        "                val = 1 if val else 0\n",
        "            # Handle nulls\n",
        "            if pd.isna(val):\n",
        "                values.append(\"NULL\")\n",
        "            # Handle strings (with escaping)\n",
        "            elif isinstance(val, str):\n",
        "                safe_val = val.replace(\"'\", \"''\")\n",
        "                values.append(f\"'{safe_val}'\")\n",
        "            # Handle numbers and others\n",
        "            else:\n",
        "                values.append(str(val))\n",
        "        values_str = ', '.join(values)\n",
        "        sql = f\"INSERT INTO [{table_name}] ({columns}) VALUES ({values_str});\"\n",
        "        insert_statements.append(sql)\n",
        "        stmt.executeUpdate(sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TenantID = \"\"\n",
        "SynapseSPNClientID = \"\"\n",
        "KeyVaultName = \"\"\n",
        "ClientIDKeyVaultSecretName = \"\"\n",
        "AzureSQLJDBCURLSecretName = \"\"\n",
        "# Step 1: Get the Azure SQL JDBC URL\n",
        "\n",
        "tokenlibrary = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary  \n",
        "\n",
        "AzureSQLjdbcurl = tokenlibrary.getSecret(f\"{KeyVaultName}\", f\"{AzureSQLJDBCURLSecretName}\", \"ls_kv_dag\")  \n",
        "\n",
        "\n",
        "# Step 2: Get the Access Token\n",
        "\n",
        "access_token,exec_statement,con = jdbc_authorization(KeyVaultName, AzureSQLjdbcurl, ClientIDKeyVaultSecretName, TenantID, SynapseSPNClientID, \"SELECT 1\") \n",
        "#stmt = con.createStatement()\n",
        "\n",
        "create_table = \"\"\"\n",
        "CREATE TABLE file_metadata (\n",
        "    doc_id NVARCHAR(100),\n",
        "    cabinet NVARCHAR(100),\n",
        "    folder_id NVARCHAR(100),\n",
        "    doc_name NVARCHAR(4000),\n",
        "    doc_number NVARCHAR(100),\n",
        "    doc_type NVARCHAR(100),\n",
        "    created_date NVARCHAR(100),\n",
        "    updated_date NVARCHAR(100),\n",
        "    created_by NVARCHAR(100),\n",
        "    facility NVARCHAR(100),\n",
        "    sub_type NVARCHAR(100),\n",
        "    sheet NVARCHAR(50),\n",
        "    major_rev NVARCHAR(50),\n",
        "    minor_rev NVARCHAR(50),\n",
        "    status_date NVARCHAR(100),\n",
        "    status NVARCHAR(50),\n",
        "    doc_date NVARCHAR(100),\n",
        "    unit NVARCHAR(50),\n",
        "    d_code NVARCHAR(50),\n",
        "    rev_index NVARCHAR(50),\n",
        "    sec_flag NVARCHAR(50),\n",
        "    r_version_label NVARCHAR(255),\n",
        "    title NVARCHAR(4000),\n",
        "    subject NVARCHAR(255),\n",
        "    authors NVARCHAR(MAX),\n",
        "    r_content_size FLOAT,\n",
        "    owner_name NVARCHAR(100),\n",
        "    r_modifier NVARCHAR(100),\n",
        "    r_access_date NVARCHAR(100),\n",
        "    a_storage_type NVARCHAR(100),\n",
        "    r_immutable_flag NVARCHAR(50),\n",
        "    active_flag NVARCHAR(10) DEFAULT 'active'\n",
        ");\n",
        " \n",
        "\"\"\"\n",
        "stmt = None  # Initialize to None\n",
        "\n",
        "data_path = spark.read.json('')\n",
        "df = data_path.toPandas()\n",
        "try:\n",
        "    print(f\"Connection object type: {type(con)}\")\n",
        "    print(\"Creating statement and executing DDL...\")\n",
        "    stmt = con.createStatement()\n",
        "    stmt.executeUpdate(create_table)\n",
        "    generate_sql_inserts(df,\"file_metadata\", stmt)\n",
        "     \n",
        "except Exception as e:\n",
        "    # Print the full Java error if it's a Py4J exception    print(\"An error occurred:\")\n",
        "    print(e)\n",
        "finally:\n",
        "    # This block ensures resources are closed even if an error occurs    print(\"Closing resources...\")\n",
        "    if stmt is not None:\n",
        "        stmt.close()\n",
        "    if con is not None:\n",
        "        con.close()\n",
        "    print(\"Resources closed.\")\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "schema = StructType([\n",
        "    StructField(\"doc_uid\", StringType(), True),\n",
        "    StructField(\"stage\", StringType(), True),\n",
        "    StructField(\"status\", StringType(), True),\n",
        "    StructField(\"event_ts\", StringType(), True),  # keep as string to avoid parsing issues\n",
        "    StructField(\"error_msg\", StringType(), True)\n",
        "])\n",
        "TenantID = \"\"\n",
        "SynapseSPNClientID = \"\"\n",
        "KeyVaultName = \"\"\n",
        "ClientIDKeyVaultSecretName = \"\"\n",
        "AzureSQLJDBCURLSecretName = \"\"\n",
        "# Step 1: Get the Azure SQL JDBC URL\n",
        "\n",
        "tokenlibrary = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary  \n",
        "\n",
        "AzureSQLjdbcurl = tokenlibrary.getSecret(f\"{KeyVaultName}\", f\"{AzureSQLJDBCURLSecretName}\", \"ls_kv_dag\")  \n",
        "\n",
        "\n",
        "access_token,exec_statement,con = jdbc_authorization(KeyVaultName, AzureSQLjdbcurl, ClientIDKeyVaultSecretName, TenantID, SynapseSPNClientID, \"SELECT 1\") \n",
        "\n",
        "create_table = \"\"\"CREATE TABLE pipeline_activity (\n",
        "    doc_uid UNIQUEIDENTIFIER NOT NULL,\n",
        "    stage NVARCHAR(100),\n",
        "    status NVARCHAR(50),\n",
        "    event_ts DATETIMEOFFSET,\n",
        "    error_msg NVARCHAR(MAX)\n",
        ");\n",
        "\n",
        " \"\"\"\n",
        "path = \"\"\n",
        "df_spark = (spark.read\n",
        "                  .option(\"multiline\", \"true\")     \n",
        "                  .schema(schema)                   \n",
        "                  .json(path))\n",
        "df = df_spark.toPandas()\n",
        "print(df)\n",
        "try:\n",
        "    print(f\"Connection object type: {type(con)}\")\n",
        "    print(\"Creating statement and executing DDL...\")\n",
        "    stmt = con.createStatement()\n",
        "    #stmt.executeUpdate(create_table)\n",
        "    generate_sql_inserts(df,\"pipeline_activity\", stmt)\n",
        "     \n",
        "except Exception as e:\n",
        "    print(e)\n",
        "finally:\n",
        "    if stmt is not None:\n",
        "        stmt.close()\n",
        "    if con is not None:\n",
        "        con.close()\n",
        "    print(\"Resources closed.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
