{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-06-27T17:25:12.8706008Z",
              "execution_start_time": null,
              "livy_statement_state": null,
              "normalized_state": "finished",
              "parent_msg_id": "f60c0e61-4884-4157-a1b1-a006b3530a6b",
              "queued_time": "2025-06-27T17:25:12.7831979Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1,
              "statement_ids": []
            },
            "text/plain": [
              "StatementMeta(, , -1, Finished, , Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "LIVY_JOB_TIMED_OUT",
          "evalue": "Livy session has failed. Session state: Dead. Error code: LIVY_JOB_TIMED_OUT. Job failed during run time with state=[dead]. Source: Unknown.",
          "output_type": "error",
          "traceback": [
            "LIVY_JOB_TIMED_OUT: Livy session has failed. Session state: Dead. Error code: LIVY_JOB_TIMED_OUT. Job failed during run time with state=[dead]. Source: Unknown."
          ]
        }
      ],
      "source": [
        "pip install  -q openai azure-core azure-search-documents azure-storage-blob tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%run Documentum/logging_utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-06-27T16:41:40.9972131Z",
              "execution_start_time": "2025-06-27T16:41:40.7338362Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "de01fdee-4775-4e54-b0eb-328b8b4e7fe1",
              "queued_time": "2025-06-27T16:41:36.3881543Z",
              "session_id": "1877",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "smallbronzefull",
              "state": "finished",
              "statement_id": 41,
              "statement_ids": [
                41
              ]
            },
            "text/plain": [
              "StatementMeta(smallbronzefull, 1877, 41, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import io\n",
        "import uuid\n",
        "import json\n",
        "import math\n",
        "import tiktoken\n",
        "from datetime import datetime\n",
        "from notebookutils import mssparkutils\n",
        "from openai import AzureOpenAI\n",
        "from datetime import datetime, timedelta\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.storage.blob import BlobServiceClient \n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from azure.search.documents.indexes.models import (\n",
        "    SearchIndex,\n",
        "    SearchField,\n",
        "    SearchFieldDataType,\n",
        "    SimpleField,\n",
        "    SearchableField,\n",
        "    SearchIndex,\n",
        "    SearchField,\n",
        "    VectorSearch,\n",
        "    VectorSearchProfile,\n",
        "    HnswAlgorithmConfiguration,\n",
        "    SemanticSearch,\n",
        "    SemanticConfiguration,\n",
        "    SemanticPrioritizedFields,\n",
        "    SemanticField\n",
        ")\n",
        "from azure.storage.blob import (\n",
        "    BlobServiceClient,\n",
        "    BlobSasPermissions,\n",
        "    generate_blob_sas,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-06-27T16:41:41.2652764Z",
              "execution_start_time": "2025-06-27T16:41:41.0116562Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "e771735f-6a9b-4422-a07f-8cd803d3abee",
              "queued_time": "2025-06-27T16:41:36.4707843Z",
              "session_id": "1877",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "smallbronzefull",
              "state": "finished",
              "statement_id": 42,
              "statement_ids": [
                42
              ]
            },
            "text/plain": [
              "StatementMeta(smallbronzefull, 1877, 42, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "service_endpoint = \"\"\n",
        "index_name       = \"jsonidx\"\n",
        "key              = \"\"\n",
        "credential       = AzureKeyCredential(key)\n",
        "vector_dims       = 1536\n",
        "algo_name         = \"hnsw-cosine\"\n",
        "profile_name      = \"openai-ada-profile\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-06-27T16:41:42.5757889Z",
              "execution_start_time": "2025-06-27T16:41:41.2792258Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "416457e1-afd3-493b-b23f-ea7918da25d3",
              "queued_time": "2025-06-27T16:41:36.5129389Z",
              "session_id": "1877",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "smallbronzefull",
              "state": "finished",
              "statement_id": 43,
              "statement_ids": [
                43
              ]
            },
            "text/plain": [
              "StatementMeta(smallbronzefull, 1877, 43, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<azure.search.documents.indexes.models._index.SearchIndex at 0x7c1f16a3e6b0>"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the index fields\n",
        "client = SearchIndexClient(service_endpoint, credential)\n",
        "fields = [\n",
        "    SimpleField(name=\"id\",   type=SearchFieldDataType.String,key=True, sortable=True, filterable=True, facetable=True),\n",
        "    SimpleField(name=\"file_name\", type=SearchFieldDataType.String),\n",
        "    \n",
        "    SimpleField(name=\"page_number\",type=SearchFieldDataType.Double,sortable=True,filterable=True,facetable=False),\n",
        "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
        "    SearchField(name=\"content_vector\",type=SearchFieldDataType.Collection(SearchFieldDataType.Single),searchable=True,vector_search_dimensions=1536,vector_search_profile_name=\"my-vector-config\"),\n",
        "    SimpleField(name=\"storage_url\",type=SearchFieldDataType.String,filterable=False,facetable=False,sortable=False)\n",
        "]\n",
        " \n",
        "vector_search = VectorSearch(\n",
        "    profiles=[VectorSearchProfile(name=\"my-vector-config\",\n",
        "                                algorithm_configuration_name=\"my-algorithms-config\")],\n",
        "    algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithms-config\")],\n",
        ")\n",
        "\n",
        "# Define semantic configuration\n",
        "semantic_config = SemanticConfiguration(\n",
        "    name=\"semantic-config\",\n",
        "    prioritized_fields=SemanticPrioritizedFields(\n",
        "        title_field=SemanticField(field_name=\"file_name\"),\n",
        "        content_fields=[SemanticField(field_name=\"content\")]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add semantic search to the index\n",
        "semantic_search = SemanticSearch(\n",
        "    default_configuration_name=\"semantic-config\",\n",
        "    configurations=[semantic_config]\n",
        ")\n",
        "\n",
        "# Update the index definition\n",
        "index = SearchIndex(\n",
        "    name=index_name,\n",
        "    fields=fields,\n",
        "    vector_search=vector_search,\n",
        "    semantic_search=semantic_search\n",
        ")\n",
        "\n",
        "client.create_or_update_index(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-06-27T16:43:11.7942347Z",
              "execution_start_time": "2025-06-27T16:41:42.5919285Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "f189f8ae-3ae7-40f8-b0dd-304f214c074a",
              "queued_time": "2025-06-27T16:41:36.5833248Z",
              "session_id": "1877",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "smallbronzefull",
              "state": "finished",
              "statement_id": 44,
              "statement_ids": [
                44
              ]
            },
            "text/plain": [
              "StatementMeta(smallbronzefull, 1877, 44, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "➜ Scanning ADLS container for JSON files …\n",
            "\n",
            "➜ Processing  12B O-6.9.2, CDP, Rev-038, PCR-24-01396, GLH.pdf.json\n",
            "Total page parsed 66\n",
            "\n",
            "➜ Processing  Lessons Learned - Information Bulletin.json\n",
            "Total page parsed 1\n",
            "\n",
            "➜ Processing  REMOVE SIGHT GLASSES FROM SERVICE P-15.3 ESP Rev-003.pdf.json\n",
            "Total page parsed 10\n",
            "\n",
            "➜ Processing 003.R0, Rev 000, RBS CYCLE 10 A2-2 SEQUENCE ESTIMATED CRITICAL POSITIONS AND NOTCH WORTH CALCULATIONS.json\n"
          ]
        },
        {
          "ename": "HttpResponseError",
          "evalue": "() The request is invalid. Details: Invalid JSON. A token was not recognized in the JSON content.\nCode: \nMessage: The request is invalid. Details: Invalid JSON. A token was not recognized in the JSON content.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[89], line 153\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m#end = datetime.utcnow()\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m#pid = str(uuid.uuid4())\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m#stage = \"Bronze to Silver\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m#status = \"Convert JSON to AI\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m#size = json_size\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m#log_activity(pid, stage, status,start,end, size)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[89], line 131\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    122\u001b[0m doc \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m:             \u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()),\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m:      blob\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_url\u001b[39m\u001b[38;5;124m\"\u001b[39m : make_sas_url(blob\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    129\u001b[0m }\n\u001b[0;32m--> 131\u001b[0m result  \u001b[38;5;241m=\u001b[39m \u001b[43msearch_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    133\u001b[0m status  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msucceeded \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✗\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.10/site-packages/azure/search/documents/_search_client.py:548\u001b[0m, in \u001b[0;36mSearchClient.upload_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m batch\u001b[38;5;241m.\u001b[39madd_upload_actions(documents)\n\u001b[1;32m    547\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_client_headers(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 548\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[IndexingResult], results)\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.10/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.10/site-packages/azure/search/documents/_search_client.py:647\u001b[0m, in \u001b[0;36mSearchClient.index_documents\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;129m@distributed_trace\u001b[39m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindex_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: IndexDocumentsBatch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[IndexingResult]:\n\u001b[1;32m    638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Specify a document operations to perform as a batch.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    :param batch: A batch of document operations to perform.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    :raises ~azure.search.documents.RequestEntityTooLargeError\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_documents_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.10/site-packages/azure/search/documents/_search_client.py:655\u001b[0m, in \u001b[0;36mSearchClient._index_documents_actions\u001b[0;34m(self, actions, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m batch \u001b[38;5;241m=\u001b[39m IndexBatch(actions\u001b[38;5;241m=\u001b[39mactions)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     batch_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[IndexingResult], batch_response\u001b[38;5;241m.\u001b[39mresults)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RequestEntityTooLargeError:\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.10/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.10/site-packages/azure/search/documents/_generated/operations/_documents_operations.py:1202\u001b[0m, in \u001b[0;36mDocumentsOperations.index\u001b[0;34m(self, batch, request_options, **kwargs)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[1;32m   1201\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[0;32m-> 1202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror)\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m   1205\u001b[0m     deserialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexDocumentsResult\u001b[39m\u001b[38;5;124m\"\u001b[39m, pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response)\n",
            "\u001b[0;31mHttpResponseError\u001b[0m: () The request is invalid. Details: Invalid JSON. A token was not recognized in the JSON content.\nCode: \nMessage: The request is invalid. Details: Invalid JSON. A token was not recognized in the JSON content."
          ]
        }
      ],
      "source": [
        "# ─── 1. CONFIG ────────────────────────────────────────────────────────────────\n",
        "AZURE_OPENAI_ENDPOINT  = \"\"\n",
        "AZURE_OPENAI_KEY       = \"\"\n",
        "AZURE_OPENAI_API_VER   = \"2024-10-21\"\n",
        "EMBED_DEPLOYMENT       = \"emb\"\n",
        " \n",
        "SEARCH_ENDPOINT        = \"\"\n",
        "SEARCH_API_KEY         = \"\"\n",
        "SEARCH_INDEX_NAME      = \"jsonidx\"\n",
        " \n",
        "ADLS_CONNECTION_STRING = \"\"  # blob endpoint\n",
        "ADLS_CONTAINER_NAME    = \"bronze\"\n",
        "ADLS_DIR_PREFIX        = \"pdf/\"\n",
        "ADLS_STORAGE_ACCOUNT  = \"\"\n",
        "ADLS_STORAGE_KEY = \"\"\n",
        "\n",
        " \n",
        "TEXT_MIN_CHARS         = 5\n",
        " \n",
        "# ─── 2. CLIENTS ───────────────────────────────────────────────────────────────\n",
        "openai_client = AzureOpenAI(\n",
        "    api_key        = AZURE_OPENAI_KEY,\n",
        "    azure_endpoint = AZURE_OPENAI_ENDPOINT,\n",
        "    api_version    = AZURE_OPENAI_API_VER,\n",
        ")\n",
        " \n",
        "search_client = SearchClient(\n",
        "    endpoint   = SEARCH_ENDPOINT,\n",
        "    index_name = SEARCH_INDEX_NAME,\n",
        "    credential = AzureKeyCredential(SEARCH_API_KEY)\n",
        ")\n",
        " \n",
        "blob_service  = BlobServiceClient.from_connection_string(ADLS_CONNECTION_STRING)\n",
        " \n",
        "container     = blob_service.get_container_client(ADLS_CONTAINER_NAME)\n",
        " \n",
        "def create_embedding(text: str, model: str = EMBED_DEPLOYMENT) -> list[float]:\n",
        "\n",
        "    return openai_client.embeddings.create(input=[text], model=model).data[0].embedding\n",
        " \n",
        "\n",
        "tokenizer = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
        "\n",
        "MAX_TOKENS = 8192\n",
        " \n",
        "def page_records_from_bytes(json_bytes: bytes):\n",
        "    \"\"\"\n",
        "    Yield (page_no, text) tuples where `text` contains:\n",
        "      - the joined `lines[]`\n",
        "      - every table rendered as \"col1: val1 | col2: val2\" per row\n",
        "      - split into sub-pages if over MAX_TOKENS\n",
        "    \"\"\"\n",
        "    data = json.loads(json_bytes.decode(\"utf-8\"))\n",
        "    if not isinstance(data, dict) or \"pages\" not in data:\n",
        "        return  # nothing to emit\n",
        "    for page in data[\"pages\"]:\n",
        "        page_no = page.get(\"page_number\", 0)\n",
        "        # 1. Extract lines\n",
        "        lines = page.get(\"lines\", [])\n",
        "        text_blocks = [\"\\n\".join(str(l).strip() for l in lines if str(l).strip())]\n",
        "        # 2. Extract tables\n",
        "        for tbl in page.get(\"tables\", []):\n",
        "            for row in tbl.get(\"rows\", []):\n",
        "                if isinstance(row, dict):\n",
        "                    row_txt = \" | \".join(f\"{k}: {v}\" for k, v in row.items())\n",
        "                    text_blocks.append(row_txt.strip())\n",
        "        # 3. Final full text block\n",
        "        full_text = \"\\n\".join(t for t in text_blocks if t).strip()\n",
        "        if not full_text:\n",
        "            continue\n",
        "        # 4. Token-aware chunking\n",
        "        tokens = tokenizer.encode(full_text)\n",
        "        total_tokens = len(tokens)\n",
        "        if total_tokens <= MAX_TOKENS:\n",
        "            yield str(page_no), full_text\n",
        "        else:\n",
        "            num_chunks = math.ceil(total_tokens / MAX_TOKENS)\n",
        "            for i in range(num_chunks):\n",
        "                start = i * MAX_TOKENS\n",
        "                end = start + MAX_TOKENS\n",
        "                chunk_tokens = tokens[start:end]\n",
        "                chunk_text = tokenizer.decode(chunk_tokens)\n",
        "                chunked_page_no = f\"{page_no}.{i+1}\"\n",
        "                yield chunked_page_no, chunk_text\n",
        " \n",
        "def make_sas_url(file_name: str) -> str:\n",
        "    sas = generate_blob_sas(\n",
        "        account_name=ADLS_STORAGE_ACCOUNT,\n",
        "        container_name=ADLS_CONTAINER_NAME,\n",
        "        blob_name=file_name,\n",
        "        account_key=ADLS_STORAGE_KEY,\n",
        "        permission=BlobSasPermissions(read=True),\n",
        "        expiry=datetime.utcnow() + timedelta(days=7)\n",
        "    )\n",
        "    return f\"https://{ADLS_STORAGE_ACCOUNT}.blob.core.usgovcloudapi.net/{ADLS_CONTAINER_NAME}/{file_name}?{sas}\"\n",
        " \n",
        "# ─── MAIN (only the loop body changes) ────────────────────────────────────────\n",
        "\n",
        "def main():\n",
        "    uploaded = skipped = 0\n",
        "    total_json_parsed = 0\n",
        "    total_json = 0\n",
        "    page_parsed = 0\n",
        "    json_size =0\n",
        "    print(\"➜ Scanning ADLS container for JSON files …\")\n",
        "    start = datetime.utcnow()\n",
        "    for blob in container.list_blobs():\n",
        "        if not blob.name.lower().endswith(\".json\"):\n",
        "            continue\n",
        " \n",
        "        print(f\"\\n➜ Processing {blob.name}\")\n",
        "\n",
        "        json_bytes = container.download_blob(blob.name).readall()\n",
        "        total_json += 1\n",
        "        json_size += len(json_bytes)\n",
        "        for page_no, text in page_records_from_bytes(json_bytes):\n",
        "\n",
        "            if len(text) < TEXT_MIN_CHARS:\n",
        "                skipped += 1\n",
        "                continue\n",
        " \n",
        "            doc = {\n",
        "                \"id\":             str(uuid.uuid4()),\n",
        "                \"file_name\":      blob.name,\n",
        "                \"page_number\":    page_no,\n",
        "                \"content\":        text,\n",
        "                \"content_vector\": create_embedding(text),\n",
        "                \"storage_url\" : make_sas_url(blob.name)\n",
        "            }\n",
        "\n",
        "            result  = search_client.upload_documents([doc])[0]\n",
        "\n",
        "            status  = \"✓\" if result.succeeded else \"✗\"\n",
        "            if(status == \"✓\"):\n",
        "                page_parsed += 1\n",
        "            uploaded += int(result.succeeded)\n",
        "        if(page_parsed > 0):\n",
        "            total_json_parsed +=1\n",
        "        print(f\"Total page parsed {page_parsed}\")\n",
        "        page_parsed = 0\n",
        " \n",
        "    print(f\"\\nDone. {uploaded} pages indexed, {skipped} pages skipped.\")\n",
        "    print(f\"Total JSON processed: {total_json_parsed}\\n Total JSON in Raw: {total_json}\" )\n",
        "    #end = datetime.utcnow()\n",
        "    #pid = str(uuid.uuid4())\n",
        "    #stage = \"Bronze to Silver\"\n",
        "    #status = \"Convert JSON to AI\"\n",
        "    #size = json_size\n",
        "    #log_activity(pid, stage, status,start,end, size)\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    main()\n",
        " "
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
