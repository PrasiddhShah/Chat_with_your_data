{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44c588",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install azure-ai-documentintelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ef8d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import io, os, json\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.serialization import AzureJSONEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edae51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ── 1. Client + paths ────────────────────────────────────────────\n",
    "key = \"1waRdyOKyqc6V4DJqGPoDY6Z1TVAKzGWMKvd52IUvFvcB9H0Y5fhJQQJ99BEACYeBjFXJ3w3AAALACOGVxC5\"\n",
    "endpoint      = \"https://usecase3docintel.cognitiveservices.azure.com/\"\n",
    " \n",
    "client = DocumentIntelligenceClient(endpoint, AzureKeyCredential(key))\n",
    " \n",
    "input_folder  = \"abfss://bronzedoc@conusecase3.dfs.core.windows.net/\"\n",
    "output_folder = \"abfss://json@conusecase3.dfs.core.windows.net/\n",
    "\"\n",
    " \n",
    "# ── 2. Read DOC/DOCX via Spark binaryFile ───────────────────────\n",
    "doc_files = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"pathGlobFilter\", \"*.docx\")\n",
    "    .load(input_folder)\n",
    ")\n",
    "\n",
    "# ── 3. Process each DOCX ─────────────────────────────────────────\n",
    "for row in doc_files.collect():\n",
    "    doc_path  = row.path\n",
    "    doc_bytes = row.content\n",
    "    print(f\"\\nProcessing: {doc_path}\")\n",
    "\n",
    "    # Analyse with prebuilt‑read (good for .docx, .doc)\n",
    "    poller = client.begin_analyze_document(\n",
    "        model_id=\"prebuilt-read\",\n",
    "        body=io.BytesIO(doc_bytes)\n",
    "    )\n",
    "    result = poller.result()\n",
    "\n",
    "    # ── Build the output dict ────────────────────────────────────\n",
    "    out = {\n",
    "        \"file\": os.path.basename(doc_path),\n",
    "        \"styles\": [],\n",
    "        \"pages\": []\n",
    "    }\n",
    "\n",
    "    # A) Create every page dict (lines only)\n",
    "    for page in result.pages:\n",
    "        page_dict = {\n",
    "            \"page_number\": page.page_number,\n",
    "            \"lines\": [line.content for line in page.lines],\n",
    "            \"tables\": []  # Optional: you can omit if not using tables\n",
    "        }\n",
    "        out[\"pages\"].append(page_dict)\n",
    "\n",
    "    # ── 4. Serialise & save to ADLS ──────────────────────────────\n",
    "    json_str = json.dumps(out, indent=4, cls=AzureJSONEncoder)\n",
    "    json_name = os.path.splitext(os.path.basename(doc_path))[0] + \".json\"\n",
    "    output_path = os.path.join(output_folder, json_name)\n",
    "\n",
    "    mssparkutils.fs.put(output_path, json_str, overwrite=True)\n",
    "    print(f\"✓ Saved extracted JSON → {output_path}\")\n",
    "\n",
    "print(\"\\nAll DOCX files processed and JSONs saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
